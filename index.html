<!DOCTYPE html><!--KAjfSHlmHNdzJZLHW2tAo--><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/haowang.jpg"/><link rel="stylesheet" href="/_next/static/css/63b13758c6860200.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-4a6fea9592d99ef7.js"/><script src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/_next/static/chunks/255-e057be79bd379a49.js" async=""></script><script src="/_next/static/chunks/main-app-005e88de6e55b10b.js" async=""></script><script src="/_next/static/chunks/185-02e5984e85404956.js" async=""></script><script src="/_next/static/chunks/619-9168df9c2a29b74b.js" async=""></script><script src="/_next/static/chunks/599-36b43173d762fc47.js" async=""></script><script src="/_next/static/chunks/app/layout-f605a9348e2bb169.js" async=""></script><script src="/_next/static/chunks/140-5ce7a5cc299b9e70.js" async=""></script><script src="/_next/static/chunks/681-68a0f630dd3881d4.js" async=""></script><script src="/_next/static/chunks/app/page-9e053bdfb45cdfbc.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><title>Hao Wang (ÁéãË±™)</title><meta name="description" content="Assistant Professor at Xi&#x27;an Jiaotong University"/><meta name="author" content="Hao Wang"/><meta name="keywords" content="Hao Wang,PhD,Research,Xi&#x27;an Jiaotong University"/><meta name="creator" content="Hao Wang"/><meta name="publisher" content="Hao Wang"/><meta property="og:title" content="Hao Wang (ÁéãË±™)"/><meta property="og:description" content="Assistant Professor at Xi&#x27;an Jiaotong University"/><meta property="og:site_name" content="Hao Wang&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Hao Wang (ÁéãË±™)"/><meta name="twitter:description" content="Assistant Professor at Xi&#x27;an Jiaotong University"/><link rel="icon" href="/favicon.svg"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div hidden=""><!--$--><!--/$--></div><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Hao Wang (ÁéãË±™)</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/projects/"><span class="relative z-10">Projects</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/teaching/"><span class="relative z-10">Teaching</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/cv/"><span class="relative z-10">CV</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-_R_5pdb_" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-80 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Hao Wang" width="256" height="320" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[20%_center]" style="color:transparent" src="/haowang.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Hao Wang</h1><p class="text-lg text-accent font-medium mb-1">Assistant Professor</p><p class="text-neutral-600 mb-2">Xi&#x27;an Jiaotong University</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><a href="https://scholar.google.com.hk/citations?user=PeY6HBkAAAAJ&amp;hl=zh-CN" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://orcid.org/0000-0001-6959-7237" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="ORCID"><svg viewBox="0 0 24 24" fill="currentColor" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"></path></svg></a></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>LLMs</div><div>Social Simulation Empowered by Agents</div><div>Intelligent Communication</div><div>Public Opinion Analysis</div></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">I am currently an <strong class="font-semibold text-primary">Assistant Professor</strong> at the <strong class="font-semibold text-primary">School of Journalism and New Media</strong> at <strong class="font-semibold text-primary">Xi&#x27;an Jiaotong University</strong>.</p>
<p class="mb-4 last:mb-0">I am recognized as an <strong class="font-semibold text-primary">Outstanding Young Talent (ÈùíÂπ¥‰ºòÁßÄ‰∫∫Êâç)</strong> of Xi&#x27;an Jiaotong University.</p>
<p class="mb-4 last:mb-0">I earned my Ph.D. from the <strong class="font-semibold text-primary">School of Computer Science</strong> at <strong class="font-semibold text-primary">Northwestern Polytechnical University</strong> (NWPU). Before that, I received B.E. degree in Computer Science and Technology at NWPU in 2019.</p>
<p class="mb-4 last:mb-0">My research interests include <strong class="font-semibold text-primary">Large Language Models (LLMs)</strong>, <strong class="font-semibold text-primary">Social Simulation Empowered by Agents</strong>, <strong class="font-semibold text-primary">Intelligent Communication</strong>, and <strong class="font-semibold text-primary">Public Opinion Analysis</strong>.</p>
<p class="mb-4 last:mb-0">Feel free to connect!</p></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/publications/">View All ‚Üí</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">PersuHSG: Adaptive Persuasion Strategy Planning for Dialogue Agents Based on Hierarchical Strategy Graph</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class=" ">Mengqi Chen</span>, </span><span><span class=" ">Bin Guo</span>, </span><span><span class="font-semibold text-accent ">Hao Wang</span>, </span><span><span class=" ">Jingqi Liu</span>, </span><span><span class=" ">Yan Liu</span>, </span><span><span class=" ">Yunji Liang</span>, </span><span><span class=" ">Peilin Li</span>, </span><span><span class=" ">Yan Pan</span>, </span><span><span class=" ">Zhiwen Yu</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">ACM Transactions on Information Systems (JCR Q1, CCF A)</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Enabling harmonious human-machine interaction with visual-context augmented dialogue system: A review</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Hao Wang</span>, </span><span><span class=" ">Bin Guo</span>, </span><span><span class=" ">Yating Zeng</span>, </span><span><span class=" ">Mengqi Chen</span>, </span><span><span class=" ">Yasan Ding</span>, </span><span><span class=" ">Ying Zhang</span>, </span><span><span class=" ">Lina Yao</span>, </span><span><span class=" ">Zhiwen Yu</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">ACM Transactions on Information Systems (JCR Q1, CCF A)</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class=" ">Xuecheng Wu</span>, </span><span><span class=" ">Heli Sun</span>, </span><span><span class=" ">Danlei Huang</span>, </span><span><span class=" ">Xinyi Yin</span>, </span><span><span class=" ">Yifan Wang</span>, </span><span><span class="font-semibold text-accent ">Hao Wang</span>, </span><span><span class=" ">Jia Zhang</span>, </span><span><span class=" ">Fei Wang</span>, </span><span><span class=" ">Peihao Guo</span>, </span><span><span class=" ">Suyu Xing</span>, </span><span><span class=" ">others</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Proceedings of the 33rd ACM International Conference on Multimedia (CCF A Conference)</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Memory-Enhanced Emotional Support Conversations with Motivation-Driven Strategy Inference</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Hao Wang</span>, </span><span><span class=" ">Bin Guo</span>, </span><span><span class=" ">Mengqi Chen</span>, </span><span><span class=" ">Yasan Ding</span>, </span><span><span class=" ">Qiuyun Zhang</span>, </span><span><span class=" ">Ying Zhang</span>, </span><span><span class=" ">Zhiwen Yu</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Joint European Conference on Machine Learning and Knowledge Discovery in Databases (CCF B Conference)</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Towards informative and diverse dialogue systems over hierarchical crowd intelligence knowledge graph</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Hao Wang</span>, </span><span><span class=" ">Bin Guo</span>, </span><span><span class=" ">Jiaqi Liu</span>, </span><span><span class=" ">Yasan Ding</span>, </span><span><span class=" ">Zhiwen Yu</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">ACM Transactions on Knowledge Discovery from Data (JCR Q1, CCF B)</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2026-02</span><p class="text-sm text-neutral-700">üéâ Our paper is accepted by ACM TOIS, CCF Rank A</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-08</span><p class="text-sm text-neutral-700">üéâ We win the first place in 1M-Deepfakes Detection Challenge (https://deepfakes1m.github.io/2025/evaluation) @ ACM Multimedia 2025</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-08</span><p class="text-sm text-neutral-700">üéâ Our paper is accepted by ACM Multimedia 2025, CCF Rank A</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-01</span><p class="text-sm text-neutral-700">üéâ Our paper is accepted by ACM Transactions on Information Systems 2025, CCF Rank A</p></div></div></section></section></div></div></div><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->December 10, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">üöÄ</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-4a6fea9592d99ef7.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7558,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"599\",\"static/chunks/599-36b43173d762fc47.js\",\"177\",\"static/chunks/app/layout-f605a9348e2bb169.js\"],\"ThemeProvider\"]\n3:I[9994,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"599\",\"static/chunks/599-36b43173d762fc47.js\",\"177\",\"static/chunks/app/layout-f605a9348e2bb169.js\"],\"default\"]\n4:I[9766,[],\"\"]\n5:I[8924,[],\"\"]\nb:I[7150,[],\"\"]\n:HL[\"/_next/static/css/63b13758c6860200.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"KAjfSHlmHNdzJZLHW2tAo\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/63b13758c6860200.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Projects\",\"type\":\"page\",\"target\":\"projects\",\"href\":\"/projects\"},{\"title\":\"Teaching\",\"type\":\"page\",\"target\":\"teaching\",\"href\":\"/teaching\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"Hao Wang (ÁéãË±™)\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],\"$L6\",\"$L7\"]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],\"$L8\"]}]}]]}]]}],{\"children\":[\"__PAGE__\",\"$L9\",{},null,false]},null,false],\"$La\",false]],\"m\":\"$undefined\",\"G\":[\"$b\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"c:I[7923,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"599\",\"static/chunks/599-36b43173d762fc47.js\",\"177\",\"static/chunks/app/layout-f605a9348e2bb169.js\"],\"default\"]\nd:I[9756,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"140\",\"static/chunks/140-5ce7a5cc299b9e70.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-68a0f630dd3881d4.js\",\"974\",\"static/chunks/app/page-9e053bdfb45cdfbc.js\"],\"default\"]\ne:I[470,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"140\",\"static/chunks/140-5ce7a5cc299b9e70.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-68a0f630dd3881d4.js\",\"974\",\"static/chunks/app/page-9e053bdfb45cdfbc.js\"],\"default\"]\nf:I[2597,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"140\",\"static/chunks/140-5ce7a5cc299b9e70.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-68a0f630dd3881d4.js\",\"974\",\"static/chunks/app/page-9e053bdfb45cdfbc.js\"],\"default\"]\n1c:I[4431,[],\"ViewportBoundary\"]\n1e:I[4431,[],\"MetadataBoundary\"]\n1f:\"$Sreact.suspense\"\n6:[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}]\n7:[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]\n8:[\"$\",\"$Lc\",null,{\"lastUpdated\":\"December 10, 2025\"}]\n10:T55b,"])</script><script>self.__next_f.push([1,"Persuasion, a vital social skill, influences beliefs, attitudes, and behaviors through conversation. Yet, current dialogue agents either rely on scenario-specific strategies, restricting their cross-context adaptability, or neglect persuasion‚Äôs logical structure. They focus on isolated strategy classification, overlooking the significance of fine-grained sequential planning for real-world scenarios. To address these limitations, inspired by basic human mental activities, we present PersuHSG, an adaptive persuasion strategy planning framework. The core idea is to conceptualize persuasion as a tripartite framework comprising cognition, affection, and volition, with each stage represented as a graph layer and principle-based strategies for efficient multi-stage persuasion. Specifically, we first develop PersuInstruct, a fine-tuning dataset to improve dialogue agents‚Äô strategic planning and response generation. Then, we propose a graph-aware planning algorithm for stage-strategy-response reasoning to generate persuasive responses for diverse scenarios. Extensive experiments confirm that PersuHSG significantly enhances the persuasiveness of large language models (LLMs), allows smaller models (e.g., 9B, 13B) to achieve competitive performance, and demonstrates the efficacy of structured strategy planning in improving model efficiency and adaptability."])</script><script>self.__next_f.push([1,"11:T713,"])</script><script>self.__next_f.push([1,"@article{chen2026persuhsg,\n  title = {PersuHSG: Adaptive Persuasion Strategy Planning for Dialogue Agents Based on Hierarchical Strategy Graph},\n  author = {Chen, Mengqi and Guo, Bin and Wang, Hao and Liu, Jingqi and Liu, Yan and Liang, Yunji and Li, Peilin and Pan, Yan and Yu, Zhiwen},\n  journal = {ACM Transactions on Information Systems (JCR Q1, CCF A)},\n  year = {2026},\n  publisher = {ACM New York, NY},\n  abstract = {Persuasion, a vital social skill, influences beliefs, attitudes, and behaviors through conversation. Yet, current dialogue agents either rely on scenario-specific strategies, restricting their cross-context adaptability, or neglect persuasion‚Äôs logical structure. They focus on isolated strategy classification, overlooking the significance of fine-grained sequential planning for real-world scenarios. To address these limitations, inspired by basic human mental activities, we present PersuHSG, an adaptive persuasion strategy planning framework. The core idea is to conceptualize persuasion as a tripartite framework comprising cognition, affection, and volition, with each stage represented as a graph layer and principle-based strategies for efficient multi-stage persuasion. Specifically, we first develop PersuInstruct, a fine-tuning dataset to improve dialogue agents‚Äô strategic planning and response generation. Then, we propose a graph-aware planning algorithm for stage-strategy-response reasoning to generate persuasive responses for diverse scenarios. Extensive experiments confirm that PersuHSG significantly enhances the persuasiveness of large language models (LLMs), allows smaller models (e.g., 9B, 13B) to achieve competitive performance, and demonstrates the efficacy of structured strategy planning in improving model efficiency and adaptability.},\n  doi = {x}\n}"])</script><script>self.__next_f.push([1,"12:T5c8,"])</script><script>self.__next_f.push([1,"The intelligent dialogue system, aiming at communicating with humans harmoniously with natural language, is brilliant for promoting the advancement of human-machine interaction in the era of artificial intelligence. With the gradually complex human-computer interaction requirements, it is difficult for traditional text-based dialogue system to meet the demands for more vivid and convenient interaction. Consequently, Visual-Context Augmented Dialogue (VAD) System, which has the potential to communicate with humans by perceiving and understanding multimodal information (i.e., visual context in images or videos, textual dialogue history), has become a predominant research paradigm. Benefiting from the consistency and complementarity between visual and textual context, VAD possesses the potential to generate engaging and context-aware responses. To depict the development of VAD, we first characterize the concept model of VAD and then present its generic system architecture to illustrate the system workflow, followed by a summary of multimodal fusion techniques. Subsequently, several research challenges and representative works are investigated, followed by the summary of authoritative benchmarks and real-world application of VAD. We conclude this article by putting forward some open issues and promising research trends for VAD, e.g., the cognitive mechanisms of human-machine dialogue under cross-modal dialogue context, mobile and lightweight deployment of VAD."])</script><script>self.__next_f.push([1,"13:T7c4,"])</script><script>self.__next_f.push([1,"@article{wang2025enabling,\n  title = {Enabling harmonious human-machine interaction with visual-context augmented dialogue system: A review},\n  author = {Wang, Hao and Guo, Bin and Zeng, Yating and Chen, Mengqi and Ding, Yasan and Zhang, Ying and Yao, Lina and Yu, Zhiwen},\n  journal = {ACM Transactions on Information Systems (JCR Q1, CCF A)},\n  volume = {43},\n  number = {3},\n  pages = {1--59},\n  year = {2025},\n  publisher = {ACM New York, NY},\n  abstract = {The intelligent dialogue system, aiming at communicating with humans harmoniously with natural language, is brilliant for promoting the advancement of human-machine interaction in the era of artificial intelligence. With the gradually complex human-computer interaction requirements, it is difficult for traditional text-based dialogue system to meet the demands for more vivid and convenient interaction. Consequently, Visual-Context Augmented Dialogue (VAD) System, which has the potential to communicate with humans by perceiving and understanding multimodal information (i.e., visual context in images or videos, textual dialogue history), has become a predominant research paradigm. Benefiting from the consistency and complementarity between visual and textual context, VAD possesses the potential to generate engaging and context-aware responses. To depict the development of VAD, we first characterize the concept model of VAD and then present its generic system architecture to illustrate the system workflow, followed by a summary of multimodal fusion techniques. Subsequently, several research challenges and representative works are investigated, followed by the summary of authoritative benchmarks and real-world application of VAD. We conclude this article by putting forward some open issues and promising research trends for VAD, e.g., the cognitive mechanisms of human-machine dialogue under cross-modal dialogue context, mobile and lightweight deployment of VAD.},\n  doi = {https://doi.org/10.1145/3715098}\n}"])</script><script>self.__next_f.push([1,"14:T502,"])</script><script>self.__next_f.push([1,"Advances in Generative AI have made video-level deepfake detection increasingly challenging, exposing the limitations of current detection techniques. In this paper, we present HOLA, our solution to the Video-Level Deepfake Detection track of 2025 1M-Deepfakes Detection Challenge. Inspired by the success of large-scale pretraining in the general domain, we first scale audio-visual selfsupervised pre-training in the multimodal video-level deepfake detection, which leverages our self-built dataset of 1.81M samples thereby leading to a unified two-stage framework. To be specific, HOLA features an iterative-aware cross-modal learning module for selective audio-visual interactions, hierarchical contextual modeling with gated aggregations under the local-global perspective, and a pyramid-like refiner for scale-aware cross-grained semantic enhancements. Moreover, we propose the pseudo supervised singal injection strategy to further boost model performance. Extensive experiments across expert models and MLLMs impressivly demonstrate the effectiveness of our proposed HOLA. We also conduct a series of ablation studies to explore the crucial design factors of our introduced components. Remarkably, our HOLA ranks 1st, outperforming the second by 0.0476 AUC on the TestA set."])</script><script>self.__next_f.push([1,"15:T723,"])</script><script>self.__next_f.push([1,"@inproceedings{wu2025hola,\n  title = {HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training},\n  author = {Wu, Xuecheng and Sun, Heli and Huang, Danlei and Yin, Xinyi and Wang, Yifan and Wang, Hao and Zhang, Jia and Wang, Fei and Guo, Peihao and Xing, Suyu and others},\n  booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia (CCF A Conference)},\n  pages = {13692--13699},\n  year = {2025},\n  abstract = {Advances in Generative AI have made video-level deepfake detection increasingly challenging, exposing the limitations of current detection techniques. In this paper, we present HOLA, our solution to the Video-Level Deepfake Detection track of 2025 1M-Deepfakes Detection Challenge. Inspired by the success of large-scale pretraining in the general domain, we first scale audio-visual selfsupervised pre-training in the multimodal video-level deepfake detection, which leverages our self-built dataset of 1.81M samples thereby leading to a unified two-stage framework. To be specific, HOLA features an iterative-aware cross-modal learning module for selective audio-visual interactions, hierarchical contextual modeling with gated aggregations under the local-global perspective, and a pyramid-like refiner for scale-aware cross-grained semantic enhancements. Moreover, we propose the pseudo supervised singal injection strategy to further boost model performance. Extensive experiments across expert models and MLLMs impressivly demonstrate the effectiveness of our proposed HOLA. We also conduct a series of ablation studies to explore the crucial design factors of our introduced components. Remarkably, our HOLA ranks 1st, outperforming the second by 0.0476 AUC on the TestA set.},\n  doi = {https://doi.org/10.1145/3746027.3761980}\n}"])</script><script>self.__next_f.push([1,"16:T5d5,"])</script><script>self.__next_f.push([1,"The prevalence of mental disorders is increasing globally, highlighting the need for effective emotional support systems. Emotional Support Conversation (ESC) systems have emerged as a promising solution, providing supportive responses to individuals experiencing emotional distress. However, existing ESC systems face challenges in deducing appropriate support strategies with motivations and modeling complex semantic patterns inherent in support strategies. To tackle these challenges, we propose MAGIC, a Memory-enhanced emotional support conversation system with motivation-driven strAteGy InferenCe. Considering potential responses in future conversations, we instruct models to deduce motivations for selecting appropriate future strategies from the prior dialogue history, by harnessing the knowledge deducing abilities of Large Language Models (LLMs). These deduced motivations serve as chain-of-thought to steer models in understanding the underlying reasons of strategy inference. Moreover, to capture the intricate human language patterns and knowledge embedded in support strategies, we introduce a strategy memory store to enhance strategy modeling, by disentangling the representations from the same-strategy responses as strategy memory. Experimental results on the ESConv dataset demonstrate that MAGIC significantly outperforms state-of-the-art baselines in both automatic and human evaluations, showcasing its effectiveness in generating empathetic and supportive responses."])</script><script>self.__next_f.push([1,"17:T7d6,"])</script><script>self.__next_f.push([1,"@inproceedings{wang2024memory,\n  title = {Memory-Enhanced Emotional Support Conversations with Motivation-Driven Strategy Inference},\n  author = {Wang, Hao and Guo, Bin and Chen, Mengqi and Ding, Yasan and Zhang, Qiuyun and Zhang, Ying and Yu, Zhiwen},\n  booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases (CCF B Conference)},\n  pages = {213--229},\n  year = {2024},\n  organization = {Springer},\n  doi = {https://doi.org/10.1007/978-3-031-70362-1_13},\n  abstract = {The prevalence of mental disorders is increasing globally, highlighting the need for effective emotional support systems. Emotional Support Conversation (ESC) systems have emerged as a promising solution, providing supportive responses to individuals experiencing emotional distress. However, existing ESC systems face challenges in deducing appropriate support strategies with motivations and modeling complex semantic patterns inherent in support strategies. To tackle these challenges, we propose MAGIC, a Memory-enhanced emotional support conversation system with motivation-driven strAteGy InferenCe. Considering potential responses in future conversations, we instruct models to deduce motivations for selecting appropriate future strategies from the prior dialogue history, by harnessing the knowledge deducing abilities of Large Language Models (LLMs). These deduced motivations serve as chain-of-thought to steer models in understanding the underlying reasons of strategy inference. Moreover, to capture the intricate human language patterns and knowledge embedded in support strategies, we introduce a strategy memory store to enhance strategy modeling, by disentangling the representations from the same-strategy responses as strategy memory. Experimental results on the ESConv dataset demonstrate that MAGIC significantly outperforms state-of-the-art baselines in both automatic and human evaluations, showcasing its effectiveness in generating empathetic and supportive responses.}\n}"])</script><script>self.__next_f.push([1,"18:T827,"])</script><script>self.__next_f.push([1,"Knowledge-enhanced dialogue systems aim at generating actually correct and coherent responses by reasoning over knowledge sources, which is a promising research trend. The truly harmonious human-agent dialogue systems need to conduct engaging conversations from three aspects as humans, namely (1) stating factual contents (e.g., records in Wikipedia), (2) conveying subjective and informative opinions about objects (e.g., user discussions on Twitter), and (3) impressing interlocutors with diverse expression styles (e.g., personalized expression habits). The existing knowledge base is a standardized and unified coding for factual knowledge, which could not portray the other two kinds of knowledge to make responses more informative and expressive diverse. To address this, we present CrowdDialog, a crowd intelligence knowledge-enhanced dialogue system, which takes advantage of ‚Äúcrowd intelligence knowledge‚Äù extracted from social media (with rich subjective descriptions and diversified expression styles) to promote the performance of dialogue systems. Firstly, to thoroughly mine and organize the crowd intelligence knowledge underlying large-scale and unstructured online contents, we elaborately design the Crowd Intelligence Knowledge Graph (CIKG) structure, including the domain commonsense subgraph, descriptive subgraph, and expressive subgraph. Secondly, to reasonably integrate heterogeneous crowd intelligence knowledge into responses while ensuring logicality and fluency, we propose the Gated Fusion with Dynamic Knowledge-Dependent (GFDD) model, which generates responses from the semantic and syntactic perspective with the context-aware knowledge gate and dynamic knowledge decoding. Finally, extensive experiments over both Chinese and English dialogue datasets demonstrate that our approach GFDD outperforms competitive baselines in terms of both automatic evaluation and human judgments. Besides, ablation studies indicate that the proposed CIKG has the potential to promote dialogue systems to generate fluent, informative, and diverse dialogue responses."])</script><script>self.__next_f.push([1,"19:T9fb,"])</script><script>self.__next_f.push([1,"@article{wang2023towards,\n  title = {Towards informative and diverse dialogue systems over hierarchical crowd intelligence knowledge graph},\n  author = {Wang, Hao and Guo, Bin and Liu, Jiaqi and Ding, Yasan and Yu, Zhiwen},\n  journal = {ACM Transactions on Knowledge Discovery from Data (JCR Q1, CCF B)},\n  volume = {17},\n  number = {7},\n  pages = {1--25},\n  year = {2023},\n  publisher = {ACM New York, NY},\n  doi = {https://doi.org/10.1145/3583758},\n  abstract = {Knowledge-enhanced dialogue systems aim at generating actually correct and coherent responses by reasoning over knowledge sources, which is a promising research trend. The truly harmonious human-agent dialogue systems need to conduct engaging conversations from three aspects as humans, namely (1) stating factual contents (e.g., records in Wikipedia), (2) conveying subjective and informative opinions about objects (e.g., user discussions on Twitter), and (3) impressing interlocutors with diverse expression styles (e.g., personalized expression habits). The existing knowledge base is a standardized and unified coding for factual knowledge, which could not portray the other two kinds of knowledge to make responses more informative and expressive diverse. To address this, we present CrowdDialog, a crowd intelligence knowledge-enhanced dialogue system, which takes advantage of ‚Äúcrowd intelligence knowledge‚Äù extracted from social media (with rich subjective descriptions and diversified expression styles) to promote the performance of dialogue systems. Firstly, to thoroughly mine and organize the crowd intelligence knowledge underlying large-scale and unstructured online contents, we elaborately design the Crowd Intelligence Knowledge Graph (CIKG) structure, including the domain commonsense subgraph, descriptive subgraph, and expressive subgraph. Secondly, to reasonably integrate heterogeneous crowd intelligence knowledge into responses while ensuring logicality and fluency, we propose the Gated Fusion with Dynamic Knowledge-Dependent (GFDD) model, which generates responses from the semantic and syntactic perspective with the context-aware knowledge gate and dynamic knowledge decoding. Finally, extensive experiments over both Chinese and English dialogue datasets demonstrate that our approach GFDD outperforms competitive baselines in terms of both automatic evaluation and human judgments. Besides, ablation studies indicate that the proposed CIKG has the potential to promote dialogue systems to generate fluent, informative, and diverse dialogue responses.}\n}"])</script><script>self.__next_f.push([1,"9:[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$Ld\",null,{\"author\":{\"name\":\"Hao Wang\",\"title\":\"Assistant Professor\",\"institution\":\"Xi'an Jiaotong University\",\"avatar\":\"/haowang.jpg\"},\"social\":{\"email\":\"haowangx@xjtu.edu.cn\",\"google_scholar\":\"https://scholar.google.com.hk/citations?user=PeY6HBkAAAAJ\u0026hl=zh-CN\",\"orcid\":\"https://orcid.org/0000-0001-6959-7237\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false},\"researchInterests\":[\"LLMs\",\"Social Simulation Empowered by Agents\",\"Intelligent Communication\",\"Public Opinion Analysis\"]}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$Le\",\"about\",{\"content\":\"I am currently an **Assistant Professor** at the **School of Journalism and New Media** at **Xi'an Jiaotong University**. \\n\\nI am recognized as an **Outstanding Young Talent (ÈùíÂπ¥‰ºòÁßÄ‰∫∫Êâç)** of Xi'an Jiaotong University.\\n\\nI earned my Ph.D. from the **School of Computer Science** at **Northwestern Polytechnical University** (NWPU). Before that, I received B.E. degree in Computer Science and Technology at NWPU in 2019.\\n\\nMy research interests include **Large Language Models (LLMs)**, **Social Simulation Empowered by Agents**, **Intelligent Communication**, and **Public Opinion Analysis**.\\n\\nFeel free to connect!\",\"title\":\"About\"}],[\"$\",\"$Lf\",\"featured_publications\",{\"publications\":[{\"id\":\"chen2026persuhsg\",\"title\":\"PersuHSG: Adaptive Persuasion Strategy Planning for Dialogue Agents Based on Hierarchical Strategy Graph\",\"authors\":[{\"name\":\"Mengqi Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Bin Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hao Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jingqi Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yan Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yunji Liang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Peilin Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yan Pan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhiwen Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2026,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"ACM Transactions on Information Systems (JCR Q1, CCF A)\",\"conference\":\"\",\"doi\":\"x\",\"abstract\":\"$10\",\"description\":\"\",\"selected\":true,\"preview\":\"TOIS-26.png\",\"bibtex\":\"$11\"},{\"id\":\"wang2025enabling\",\"title\":\"Enabling harmonious human-machine interaction with visual-context augmented dialogue system: A review\",\"authors\":[{\"name\":\"Hao Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Bin Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yating Zeng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mengqi Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yasan Ding\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ying Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lina Yao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhiwen Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"ACM Transactions on Information Systems (JCR Q1, CCF A)\",\"conference\":\"\",\"volume\":\"43\",\"issue\":\"3\",\"pages\":\"1--59\",\"doi\":\"https://doi.org/10.1145/3715098\",\"abstract\":\"$12\",\"description\":\"\",\"selected\":true,\"preview\":\"TOIS25.jpg\",\"bibtex\":\"$13\"},{\"id\":\"wu2025hola\",\"title\":\"HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training\",\"authors\":[{\"name\":\"Xuecheng Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Heli Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Danlei Huang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xinyi Yin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yifan Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hao Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jia Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fei Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Peihao Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Suyu Xing\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"others\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 33rd ACM International Conference on Multimedia (CCF A Conference)\",\"pages\":\"13692--13699\",\"doi\":\"https://doi.org/10.1145/3746027.3761980\",\"abstract\":\"$14\",\"description\":\"\",\"selected\":true,\"preview\":\"MM25-HOLA.png\",\"bibtex\":\"$15\"},{\"id\":\"wang2024memory\",\"title\":\"Memory-Enhanced Emotional Support Conversations with Motivation-Driven Strategy Inference\",\"authors\":[{\"name\":\"Hao Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Bin Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mengqi Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yasan Ding\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qiuyun Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ying Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhiwen Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Joint European Conference on Machine Learning and Knowledge Discovery in Databases (CCF B Conference)\",\"pages\":\"213--229\",\"doi\":\"https://doi.org/10.1007/978-3-031-70362-1_13\",\"abstract\":\"$16\",\"description\":\"\",\"selected\":true,\"preview\":\"Model_Architecture_ECML.jpg\",\"bibtex\":\"$17\"},{\"id\":\"wang2023towards\",\"title\":\"Towards informative and diverse dialogue systems over hierarchical crowd intelligence knowledge graph\",\"authors\":[{\"name\":\"Hao Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Bin Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaqi Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yasan Ding\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhiwen Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:4:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"ACM Transactions on Knowledge Discovery from Data (JCR Q1, CCF B)\",\"conference\":\"\",\"volume\":\"17\",\"issue\":\"7\",\"pages\":\"1--25\",\"doi\":\"https://doi.org/10.1145/3583758\",\"abstract\":\"$18\",\"description\":\"\",\"selected\":true,\"preview\":\"Architecture-TKDD.png\",\"bibtex\":\"$19\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":false}],\"$L1a\"],false,false,false]}]]}]]}]}],null,\"$L1b\"]}]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L1c\",null,{\"children\":\"$L1d\"}],null],[\"$\",\"$L1e\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$1f\",null,{\"fallback\":null,\"children\":\"$L20\"}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"21:I[5078,[\"185\",\"static/chunks/185-02e5984e85404956.js\",\"140\",\"static/chunks/140-5ce7a5cc299b9e70.js\",\"619\",\"static/chunks/619-9168df9c2a29b74b.js\",\"681\",\"static/chunks/681-68a0f630dd3881d4.js\",\"974\",\"static/chunks/app/page-9e053bdfb45cdfbc.js\"],\"default\"]\n22:I[4431,[],\"OutletBoundary\"]\n24:I[5278,[],\"AsyncMetadataOutlet\"]\n1a:[\"$\",\"$L21\",\"news\",{\"items\":[{\"date\":\"2026-02\",\"content\":\"üéâ Our paper is accepted by ACM TOIS, CCF Rank A\"},{\"date\":\"2025-08\",\"content\":\"üéâ We win the first place in 1M-Deepfakes Detection Challenge (https://deepfakes1m.github.io/2025/evaluation) @ ACM Multimedia 2025\"},{\"date\":\"2025-08\",\"content\":\"üéâ Our paper is accepted by ACM Multimedia 2025, CCF Rank A\"},{\"date\":\"2025-01\",\"content\":\"üéâ Our paper is accepted by ACM Transactions on Information Systems 2025, CCF Rank A\"}],\"title\":\"News\"}]\n1b:[\"$\",\"$L22\",null,{\"children\":[\"$L23\",[\"$\",\"$L24\",null,{\"promise\":\"$@25\"}]]}]\n"])</script><script>self.__next_f.push([1,"1d:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"23:null\n"])</script><script>self.__next_f.push([1,"26:I[622,[],\"IconMark\"]\n"])</script><script>self.__next_f.push([1,"25:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Hao Wang (ÁéãË±™)\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Assistant Professor at Xi'an Jiaotong University\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Hao Wang\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Hao Wang,PhD,Research,Xi'an Jiaotong University\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Hao Wang\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Hao Wang\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Hao Wang (ÁéãË±™)\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Assistant Professor at Xi'an Jiaotong University\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Hao Wang's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Hao Wang (ÁéãË±™)\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Assistant Professor at Xi'an Jiaotong University\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}],[\"$\",\"$L26\",\"15\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"20:\"$25:metadata\"\n"])</script></body></html>