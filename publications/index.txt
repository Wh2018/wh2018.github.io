1:"$Sreact.fragment"
2:I[7558,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-f605a9348e2bb169.js"],"ThemeProvider"]
3:I[9994,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-f605a9348e2bb169.js"],"default"]
4:I[9766,[],""]
5:I[8924,[],""]
c:I[7150,[],""]
:HL["/_next/static/css/63b13758c6860200.css","style"]
0:{"P":null,"b":"KAjfSHlmHNdzJZLHW2tAo","p":"","c":["","publications",""],"i":false,"f":[[["",{"children":[["slug","publications","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/63b13758c6860200.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Projects","type":"page","target":"projects","href":"/projects"},{"title":"Teaching","type":"page","target":"teaching","href":"/teaching"},{"title":"Awards","type":"page","target":"awards","href":"/awards"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Hao Wang (王豪)","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],"$L6","$L7"]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],"$L8"]}]}]]}]]}],{"children":[["slug","publications","d"],"$L9",{"children":["__PAGE__","$La",{},null,false]},null,false]},null,false],"$Lb",false]],"m":"$undefined","G":["$c",[]],"s":false,"S":true}
d:I[7923,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-f605a9348e2bb169.js"],"default"]
f:I[4431,[],"OutletBoundary"]
11:I[5278,[],"AsyncMetadataOutlet"]
13:I[4431,[],"ViewportBoundary"]
15:I[4431,[],"MetadataBoundary"]
16:"$Sreact.suspense"
6:["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}]
7:["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]
8:["$","$Ld",null,{"lastUpdated":"December 10, 2025"}]
9:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
a:["$","$1","c",{"children":["$Le",null,["$","$Lf",null,{"children":["$L10",["$","$L11",null,{"promise":"$@12"}]]}]]}]
b:["$","$1","h",{"children":[null,[["$","$L13",null,{"children":"$L14"}],null],["$","$L15",null,{"children":["$","div",null,{"hidden":true,"children":["$","$16",null,{"fallback":null,"children":"$L17"}]}]}]]}]
18:I[9958,["185","static/chunks/185-02e5984e85404956.js","140","static/chunks/140-5ce7a5cc299b9e70.js","681","static/chunks/681-68a0f630dd3881d4.js","182","static/chunks/app/%5Bslug%5D/page-cb003bb65a33a78b.js"],"default"]
19:T55b,Persuasion, a vital social skill, influences beliefs, attitudes, and behaviors through conversation. Yet, current dialogue agents either rely on scenario-specific strategies, restricting their cross-context adaptability, or neglect persuasion’s logical structure. They focus on isolated strategy classification, overlooking the significance of fine-grained sequential planning for real-world scenarios. To address these limitations, inspired by basic human mental activities, we present PersuHSG, an adaptive persuasion strategy planning framework. The core idea is to conceptualize persuasion as a tripartite framework comprising cognition, affection, and volition, with each stage represented as a graph layer and principle-based strategies for efficient multi-stage persuasion. Specifically, we first develop PersuInstruct, a fine-tuning dataset to improve dialogue agents’ strategic planning and response generation. Then, we propose a graph-aware planning algorithm for stage-strategy-response reasoning to generate persuasive responses for diverse scenarios. Extensive experiments confirm that PersuHSG significantly enhances the persuasiveness of large language models (LLMs), allows smaller models (e.g., 9B, 13B) to achieve competitive performance, and demonstrates the efficacy of structured strategy planning in improving model efficiency and adaptability.1a:T713,@article{chen2026persuhsg,
  title = {PersuHSG: Adaptive Persuasion Strategy Planning for Dialogue Agents Based on Hierarchical Strategy Graph},
  author = {Chen, Mengqi and Guo, Bin and Wang, Hao and Liu, Jingqi and Liu, Yan and Liang, Yunji and Li, Peilin and Pan, Yan and Yu, Zhiwen},
  journal = {ACM Transactions on Information Systems (JCR Q1, CCF A)},
  year = {2026},
  publisher = {ACM New York, NY},
  abstract = {Persuasion, a vital social skill, influences beliefs, attitudes, and behaviors through conversation. Yet, current dialogue agents either rely on scenario-specific strategies, restricting their cross-context adaptability, or neglect persuasion’s logical structure. They focus on isolated strategy classification, overlooking the significance of fine-grained sequential planning for real-world scenarios. To address these limitations, inspired by basic human mental activities, we present PersuHSG, an adaptive persuasion strategy planning framework. The core idea is to conceptualize persuasion as a tripartite framework comprising cognition, affection, and volition, with each stage represented as a graph layer and principle-based strategies for efficient multi-stage persuasion. Specifically, we first develop PersuInstruct, a fine-tuning dataset to improve dialogue agents’ strategic planning and response generation. Then, we propose a graph-aware planning algorithm for stage-strategy-response reasoning to generate persuasive responses for diverse scenarios. Extensive experiments confirm that PersuHSG significantly enhances the persuasiveness of large language models (LLMs), allows smaller models (e.g., 9B, 13B) to achieve competitive performance, and demonstrates the efficacy of structured strategy planning in improving model efficiency and adaptability.},
  doi = {x}
}1b:T5c8,The intelligent dialogue system, aiming at communicating with humans harmoniously with natural language, is brilliant for promoting the advancement of human-machine interaction in the era of artificial intelligence. With the gradually complex human-computer interaction requirements, it is difficult for traditional text-based dialogue system to meet the demands for more vivid and convenient interaction. Consequently, Visual-Context Augmented Dialogue (VAD) System, which has the potential to communicate with humans by perceiving and understanding multimodal information (i.e., visual context in images or videos, textual dialogue history), has become a predominant research paradigm. Benefiting from the consistency and complementarity between visual and textual context, VAD possesses the potential to generate engaging and context-aware responses. To depict the development of VAD, we first characterize the concept model of VAD and then present its generic system architecture to illustrate the system workflow, followed by a summary of multimodal fusion techniques. Subsequently, several research challenges and representative works are investigated, followed by the summary of authoritative benchmarks and real-world application of VAD. We conclude this article by putting forward some open issues and promising research trends for VAD, e.g., the cognitive mechanisms of human-machine dialogue under cross-modal dialogue context, mobile and lightweight deployment of VAD.1c:T7c4,@article{wang2025enabling,
  title = {Enabling harmonious human-machine interaction with visual-context augmented dialogue system: A review},
  author = {Wang, Hao and Guo, Bin and Zeng, Yating and Chen, Mengqi and Ding, Yasan and Zhang, Ying and Yao, Lina and Yu, Zhiwen},
  journal = {ACM Transactions on Information Systems (JCR Q1, CCF A)},
  volume = {43},
  number = {3},
  pages = {1--59},
  year = {2025},
  publisher = {ACM New York, NY},
  abstract = {The intelligent dialogue system, aiming at communicating with humans harmoniously with natural language, is brilliant for promoting the advancement of human-machine interaction in the era of artificial intelligence. With the gradually complex human-computer interaction requirements, it is difficult for traditional text-based dialogue system to meet the demands for more vivid and convenient interaction. Consequently, Visual-Context Augmented Dialogue (VAD) System, which has the potential to communicate with humans by perceiving and understanding multimodal information (i.e., visual context in images or videos, textual dialogue history), has become a predominant research paradigm. Benefiting from the consistency and complementarity between visual and textual context, VAD possesses the potential to generate engaging and context-aware responses. To depict the development of VAD, we first characterize the concept model of VAD and then present its generic system architecture to illustrate the system workflow, followed by a summary of multimodal fusion techniques. Subsequently, several research challenges and representative works are investigated, followed by the summary of authoritative benchmarks and real-world application of VAD. We conclude this article by putting forward some open issues and promising research trends for VAD, e.g., the cognitive mechanisms of human-machine dialogue under cross-modal dialogue context, mobile and lightweight deployment of VAD.},
  doi = {https://doi.org/10.1145/3715098}
}1d:T502,Advances in Generative AI have made video-level deepfake detection increasingly challenging, exposing the limitations of current detection techniques. In this paper, we present HOLA, our solution to the Video-Level Deepfake Detection track of 2025 1M-Deepfakes Detection Challenge. Inspired by the success of large-scale pretraining in the general domain, we first scale audio-visual selfsupervised pre-training in the multimodal video-level deepfake detection, which leverages our self-built dataset of 1.81M samples thereby leading to a unified two-stage framework. To be specific, HOLA features an iterative-aware cross-modal learning module for selective audio-visual interactions, hierarchical contextual modeling with gated aggregations under the local-global perspective, and a pyramid-like refiner for scale-aware cross-grained semantic enhancements. Moreover, we propose the pseudo supervised singal injection strategy to further boost model performance. Extensive experiments across expert models and MLLMs impressivly demonstrate the effectiveness of our proposed HOLA. We also conduct a series of ablation studies to explore the crucial design factors of our introduced components. Remarkably, our HOLA ranks 1st, outperforming the second by 0.0476 AUC on the TestA set.1e:T723,@inproceedings{wu2025hola,
  title = {HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training},
  author = {Wu, Xuecheng and Sun, Heli and Huang, Danlei and Yin, Xinyi and Wang, Yifan and Wang, Hao and Zhang, Jia and Wang, Fei and Guo, Peihao and Xing, Suyu and others},
  booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia (CCF A Conference)},
  pages = {13692--13699},
  year = {2025},
  abstract = {Advances in Generative AI have made video-level deepfake detection increasingly challenging, exposing the limitations of current detection techniques. In this paper, we present HOLA, our solution to the Video-Level Deepfake Detection track of 2025 1M-Deepfakes Detection Challenge. Inspired by the success of large-scale pretraining in the general domain, we first scale audio-visual selfsupervised pre-training in the multimodal video-level deepfake detection, which leverages our self-built dataset of 1.81M samples thereby leading to a unified two-stage framework. To be specific, HOLA features an iterative-aware cross-modal learning module for selective audio-visual interactions, hierarchical contextual modeling with gated aggregations under the local-global perspective, and a pyramid-like refiner for scale-aware cross-grained semantic enhancements. Moreover, we propose the pseudo supervised singal injection strategy to further boost model performance. Extensive experiments across expert models and MLLMs impressivly demonstrate the effectiveness of our proposed HOLA. We also conduct a series of ablation studies to explore the crucial design factors of our introduced components. Remarkably, our HOLA ranks 1st, outperforming the second by 0.0476 AUC on the TestA set.},
  doi = {https://doi.org/10.1145/3746027.3761980}
}1f:T666,Video-Grounded Dialogue System (VGDS), focusing on generating reasonable responses based on multiturn dialogue contexts and a given video, has received intensive attention recently. The key to building a superior VGDS lies in efficiently reasoning over visual and textual concepts of various granularities and achieving comprehensive visual-textual multimodality alignment. Despite remarkable research progress, existing studies suffer from identifying context-relevant video parts while disregarding the impact of redundant information in long-form and content-dynamic videos. Further, current methods usually align all semantics in different modalities uniformly using a one-time cross-attention scheme, which neglects the sophisticated correspondence between various granularities of visual and textual concepts (e.g., still objects with nouns, dynamic events with verbs). To this end, we propose a novel system, namely Cascade cOntext-oriented Spatio-Temporal Attention Network (COSTA), to generate reasonable responses efficiently and accurately. Specifically, COSTA first adopts a cascade attention network to localize only the most relevant video clips and regions in a coarse-tofine manner which effectively filters the irrelevant visual semantics. Secondly, we design a memory distillation-inspired iterative visual-textual cross-attention strategy to progressively integrate visual semantics with dialogue contexts across varying granularities, facilitating extensive multi-modal alignment. Experiments on several benchmarks demonstrate significant improvements in our model over state-of-the-art methods across various metrics.20:T85b,@article{wang2025cascade,
  title = {Cascade context-oriented spatio-temporal attention network for efficient and fine-grained video-grounded dialogues},
  author = {Wang, Hao and Guo, Bin and Chen, Mengqi and Zhang, Qiuyun and Ding, Yasan and Zhang, Ying and Yu, Zhiwen},
  journal = {Frontiers of Computer Science (JCR Q1, CCF B)},
  volume = {19},
  number = {7},
  pages = {197329},
  year = {2025},
  publisher = {Springer},
  abstract = {Video-Grounded Dialogue System (VGDS), focusing on generating reasonable responses based on multiturn dialogue contexts and a given video, has received intensive attention recently. The key to building a superior VGDS lies in efficiently reasoning over visual and textual concepts of various granularities and achieving comprehensive visual-textual multimodality alignment. Despite remarkable research progress, existing studies suffer from identifying context-relevant video parts while disregarding the impact of redundant information in long-form and content-dynamic videos. Further, current methods usually align all semantics in different modalities uniformly using a one-time cross-attention scheme, which neglects the sophisticated correspondence between various granularities of visual and textual concepts (e.g., still objects with nouns, dynamic events with verbs). To this end, we propose a novel system, namely Cascade cOntext-oriented Spatio-Temporal Attention Network (COSTA), to generate reasonable responses efficiently and accurately. Specifically, COSTA first adopts a cascade attention network to localize only the most relevant video clips and regions in a coarse-tofine manner which effectively filters the irrelevant visual semantics. Secondly, we design a memory distillation-inspired iterative visual-textual cross-attention strategy to progressively integrate visual semantics with dialogue contexts across varying granularities, facilitating extensive multi-modal alignment. Experiments on several benchmarks demonstrate significant improvements in our model over state-of-the-art methods across various metrics.},
  doi = {https://doi.org/10.1007/s11704-024-40387-w}
}21:T679,Persuasion, as one of the crucial abilities in human communication, has garnered extensive attention from researchers within the field of intelligent dialogue systems. Developing dialogue agents that can persuade others to accept certain standpoints is essential to achieving truly intelligent and anthropomorphic dialogue systems. Benefiting from the substantial progress of Large Language Models (LLMs), dialogue agents have acquired an exceptional capability in context understanding and response generation. However, as a typical and complicated cognitive psychological system, persuasive dialogue agents also require knowledge from the domain of cognitive psychology to attain a level of human-like persuasion. Consequently, the cognitive strategy-enhanced persuasive dialogue agent (defined as ), which incorporates cognitive strategies to achieve persuasive targets through conversation, has become a predominant research paradigm. To depict the research trends of CogAgent, in this paper, we first present several fundamental cognitive psychology theories and give the formalized definition of three typical cognitive strategies, including the persuasion strategy, the topic path planning strategy, and the argument structure prediction strategy. Then we propose a new system architecture by incorporating the formalized definition to lay the foundation of CogAgent. Representative works are detailed and investigated according to the combined cognitive strategy, followed by the summary of authoritative benchmarks and evaluation metrics. Finally, we summarize our insights on open issues and future directions of CogAgent for upcoming researchers.22:T874,@article{chen2025future,
  title = {The future of cognitive strategy-enhanced persuasive dialogue agents: new perspectives and trends},
  author = {Chen, Mengqi and Guo, Bin and Wang, Hao and Li, Haoyu and Zhao, Qian and Liu, Jingqi and Ding, Yasan and Pan, Yan and Yu, Zhiwen},
  journal = {Frontiers of Computer Science (JCR Q1, CCF B)},
  volume = {19},
  number = {5},
  pages = {195315},
  year = {2025},
  publisher = {Springer},
  abstract = {Persuasion, as one of the crucial abilities in human communication, has garnered extensive attention from researchers within the field of intelligent dialogue systems. Developing dialogue agents that can persuade others to accept certain standpoints is essential to achieving truly intelligent and anthropomorphic dialogue systems. Benefiting from the substantial progress of Large Language Models (LLMs), dialogue agents have acquired an exceptional capability in context understanding and response generation. However, as a typical and complicated cognitive psychological system, persuasive dialogue agents also require knowledge from the domain of cognitive psychology to attain a level of human-like persuasion. Consequently, the cognitive strategy-enhanced persuasive dialogue agent (defined as ), which incorporates cognitive strategies to achieve persuasive targets through conversation, has become a predominant research paradigm. To depict the research trends of CogAgent, in this paper, we first present several fundamental cognitive psychology theories and give the formalized definition of three typical cognitive strategies, including the persuasion strategy, the topic path planning strategy, and the argument structure prediction strategy. Then we propose a new system architecture by incorporating the formalized definition to lay the foundation of CogAgent. Representative works are detailed and investigated according to the combined cognitive strategy, followed by the summary of authoritative benchmarks and evaluation metrics. Finally, we summarize our insights on open issues and future directions of CogAgent for upcoming researchers.},
  doi = {https://doi.org/10.1007/s11704-024-40057-x}
}23:T5d5,The prevalence of mental disorders is increasing globally, highlighting the need for effective emotional support systems. Emotional Support Conversation (ESC) systems have emerged as a promising solution, providing supportive responses to individuals experiencing emotional distress. However, existing ESC systems face challenges in deducing appropriate support strategies with motivations and modeling complex semantic patterns inherent in support strategies. To tackle these challenges, we propose MAGIC, a Memory-enhanced emotional support conversation system with motivation-driven strAteGy InferenCe. Considering potential responses in future conversations, we instruct models to deduce motivations for selecting appropriate future strategies from the prior dialogue history, by harnessing the knowledge deducing abilities of Large Language Models (LLMs). These deduced motivations serve as chain-of-thought to steer models in understanding the underlying reasons of strategy inference. Moreover, to capture the intricate human language patterns and knowledge embedded in support strategies, we introduce a strategy memory store to enhance strategy modeling, by disentangling the representations from the same-strategy responses as strategy memory. Experimental results on the ESConv dataset demonstrate that MAGIC significantly outperforms state-of-the-art baselines in both automatic and human evaluations, showcasing its effectiveness in generating empathetic and supportive responses.24:T7d6,@inproceedings{wang2024memory,
  title = {Memory-Enhanced Emotional Support Conversations with Motivation-Driven Strategy Inference},
  author = {Wang, Hao and Guo, Bin and Chen, Mengqi and Ding, Yasan and Zhang, Qiuyun and Zhang, Ying and Yu, Zhiwen},
  booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases (CCF B Conference)},
  pages = {213--229},
  year = {2024},
  organization = {Springer},
  doi = {https://doi.org/10.1007/978-3-031-70362-1_13},
  abstract = {The prevalence of mental disorders is increasing globally, highlighting the need for effective emotional support systems. Emotional Support Conversation (ESC) systems have emerged as a promising solution, providing supportive responses to individuals experiencing emotional distress. However, existing ESC systems face challenges in deducing appropriate support strategies with motivations and modeling complex semantic patterns inherent in support strategies. To tackle these challenges, we propose MAGIC, a Memory-enhanced emotional support conversation system with motivation-driven strAteGy InferenCe. Considering potential responses in future conversations, we instruct models to deduce motivations for selecting appropriate future strategies from the prior dialogue history, by harnessing the knowledge deducing abilities of Large Language Models (LLMs). These deduced motivations serve as chain-of-thought to steer models in understanding the underlying reasons of strategy inference. Moreover, to capture the intricate human language patterns and knowledge embedded in support strategies, we introduce a strategy memory store to enhance strategy modeling, by disentangling the representations from the same-strategy responses as strategy memory. Experimental results on the ESConv dataset demonstrate that MAGIC significantly outperforms state-of-the-art baselines in both automatic and human evaluations, showcasing its effectiveness in generating empathetic and supportive responses.}
}25:T827,Knowledge-enhanced dialogue systems aim at generating actually correct and coherent responses by reasoning over knowledge sources, which is a promising research trend. The truly harmonious human-agent dialogue systems need to conduct engaging conversations from three aspects as humans, namely (1) stating factual contents (e.g., records in Wikipedia), (2) conveying subjective and informative opinions about objects (e.g., user discussions on Twitter), and (3) impressing interlocutors with diverse expression styles (e.g., personalized expression habits). The existing knowledge base is a standardized and unified coding for factual knowledge, which could not portray the other two kinds of knowledge to make responses more informative and expressive diverse. To address this, we present CrowdDialog, a crowd intelligence knowledge-enhanced dialogue system, which takes advantage of “crowd intelligence knowledge” extracted from social media (with rich subjective descriptions and diversified expression styles) to promote the performance of dialogue systems. Firstly, to thoroughly mine and organize the crowd intelligence knowledge underlying large-scale and unstructured online contents, we elaborately design the Crowd Intelligence Knowledge Graph (CIKG) structure, including the domain commonsense subgraph, descriptive subgraph, and expressive subgraph. Secondly, to reasonably integrate heterogeneous crowd intelligence knowledge into responses while ensuring logicality and fluency, we propose the Gated Fusion with Dynamic Knowledge-Dependent (GFDD) model, which generates responses from the semantic and syntactic perspective with the context-aware knowledge gate and dynamic knowledge decoding. Finally, extensive experiments over both Chinese and English dialogue datasets demonstrate that our approach GFDD outperforms competitive baselines in terms of both automatic evaluation and human judgments. Besides, ablation studies indicate that the proposed CIKG has the potential to promote dialogue systems to generate fluent, informative, and diverse dialogue responses.26:T9fb,@article{wang2023towards,
  title = {Towards informative and diverse dialogue systems over hierarchical crowd intelligence knowledge graph},
  author = {Wang, Hao and Guo, Bin and Liu, Jiaqi and Ding, Yasan and Yu, Zhiwen},
  journal = {ACM Transactions on Knowledge Discovery from Data (JCR Q1, CCF B)},
  volume = {17},
  number = {7},
  pages = {1--25},
  year = {2023},
  publisher = {ACM New York, NY},
  doi = {https://doi.org/10.1145/3583758},
  abstract = {Knowledge-enhanced dialogue systems aim at generating actually correct and coherent responses by reasoning over knowledge sources, which is a promising research trend. The truly harmonious human-agent dialogue systems need to conduct engaging conversations from three aspects as humans, namely (1) stating factual contents (e.g., records in Wikipedia), (2) conveying subjective and informative opinions about objects (e.g., user discussions on Twitter), and (3) impressing interlocutors with diverse expression styles (e.g., personalized expression habits). The existing knowledge base is a standardized and unified coding for factual knowledge, which could not portray the other two kinds of knowledge to make responses more informative and expressive diverse. To address this, we present CrowdDialog, a crowd intelligence knowledge-enhanced dialogue system, which takes advantage of “crowd intelligence knowledge” extracted from social media (with rich subjective descriptions and diversified expression styles) to promote the performance of dialogue systems. Firstly, to thoroughly mine and organize the crowd intelligence knowledge underlying large-scale and unstructured online contents, we elaborately design the Crowd Intelligence Knowledge Graph (CIKG) structure, including the domain commonsense subgraph, descriptive subgraph, and expressive subgraph. Secondly, to reasonably integrate heterogeneous crowd intelligence knowledge into responses while ensuring logicality and fluency, we propose the Gated Fusion with Dynamic Knowledge-Dependent (GFDD) model, which generates responses from the semantic and syntactic perspective with the context-aware knowledge gate and dynamic knowledge decoding. Finally, extensive experiments over both Chinese and English dialogue datasets demonstrate that our approach GFDD outperforms competitive baselines in terms of both automatic evaluation and human judgments. Besides, ablation studies indicate that the proposed CIKG has the potential to promote dialogue systems to generate fluent, informative, and diverse dialogue responses.}
}27:T7d7,Explainability is crucial for the successful use of AI for fake news detection (FND). Researchers aim to improve the explainability of FND by highlighting important descriptions in crowd-contributed comments as clues. From the perspective of law and sociology, there are distinct clues that are easy to discover and understand, and faint clues that require careful observation and analysis. For example, in fake news related to COVID-Omicron showing increased pathogenicity and transmissibility, distinct clues might involve virologists’ opinions regarding the inverse correlation between pathogenicity and transmissibility. Meanwhile, faint clues might be reflected in an infected person’s claim that the symptoms are milder than a cold (indirectly indicating reduced pathogenicity). Occasionally, the statements of some ordinary eyewitnesses can decisively reveal the truth of the news, leading to the judgment of fake news. Existing methods generally use static networks to model the entire news life-cycle, which makes it fail to capture the subtle dynamic interactions between individual clues and news. Thereby faint clues, whose relations to the truth of news are challenging to be characterized and extracted directly, are more likely to be overshadowed by distinct clues. To address this issue, we propose an explainable FND method, dubbed as PiercingEye, which leverages dynamic interaction information to progressively mine valuable clues. PiercingEye models the news propagation topology as a dynamic graph, with interactive comments serving as nodes, and employs the time-semantic encoding mechanism to refine the modeling of temporal interaction information between comments and news to preserve faint clues. Subsequently, it utilizes the self-attention mechanism to aggregate distinct and faint clues for FND. Experimental results demonstrate that PiercingEye outperforms state-of-the-art methods and is capable of identifying both faint and distinct clues for humans to debunk fake news.28:T996,@incollection{ding2023piercingeye,
  title = {PiercingEye: Identifying Both Faint and Distinct Clues for Explainable Fake News Detection with Progressive Dynamic Graph Mining},
  author = {Ding, Yasan and Guo, Bin and Liu, Yan and Wang, Hao and Shen, Haocheng and Yu, Zhiwen},
  booktitle = {ECAI (CCF B Conference)},
  pages = {549--556},
  year = {2023},
  publisher = {IOS Press},
  abstract = {Explainability is crucial for the successful use of AI for fake news detection (FND). Researchers aim to improve the explainability of FND by highlighting important descriptions in crowd-contributed comments as clues. From the perspective of law and sociology, there are distinct clues that are easy to discover and understand, and faint clues that require careful observation and analysis. For example, in fake news related to COVID-Omicron showing increased pathogenicity and transmissibility, distinct clues might involve virologists’ opinions regarding the inverse correlation between pathogenicity and transmissibility. Meanwhile, faint clues might be reflected in an infected person’s claim that the symptoms are milder than a cold (indirectly indicating reduced pathogenicity). Occasionally, the statements of some ordinary eyewitnesses can decisively reveal the truth of the news, leading to the judgment of fake news. Existing methods generally use static networks to model the entire news life-cycle, which makes it fail to capture the subtle dynamic interactions between individual clues and news. Thereby faint clues, whose relations to the truth of news are challenging to be characterized and extracted directly, are more likely to be overshadowed by distinct clues. To address this issue, we propose an explainable FND method, dubbed as PiercingEye, which leverages dynamic interaction information to progressively mine valuable clues. PiercingEye models the news propagation topology as a dynamic graph, with interactive comments serving as nodes, and employs the time-semantic encoding mechanism to refine the modeling of temporal interaction information between comments and news to preserve faint clues. Subsequently, it utilizes the self-attention mechanism to aggregate distinct and faint clues for FND. Experimental results demonstrate that PiercingEye outperforms state-of-the-art methods and is capable of identifying both faint and distinct clues for humans to debunk fake news.},
  doi = {https://doi.org/10.3233/FAIA230315}
}29:T43a,Dialogue systems have made massive promising progress contributed by deep learning techniques and have been widely applied in our life. However, existing end-to-end neural models suffer from the problem of tending to generate uninformative and generic responses because they cannot ground dialogue context with background knowledge. In order to solve this problem, many researchers begin to consider combining external knowledge in dialogue systems, namely knowledge-enhanced dialogue systems. The challenges of knowledge-enhanced dialogue systems include how to select the appropriate knowledge from large-scale knowledge bases, how to read and understand extracted knowledge, and how to integrate knowledge into responses generation process. Combined with external knowledge, dialogue systems can deeply understand the dialogue context, and generate more informative and logical responses. This survey gives a comprehensive review of knowledge-enhanced dialogue systems, summarizes research progress to solve these challenges and proposes some open issues and research directions.2a:T5d4,@article{wang2021towards,
  title = {Towards information-rich, logical dialogue systems with knowledge-enhanced neural models},
  author = {Wang, Hao and Guo, Bin and Wu, Wei and Liu, Sicong and Yu, Zhiwen},
  journal = {Neurocomputing (JCR Q1, CCF B)},
  volume = {465},
  pages = {248--264},
  year = {2021},
  publisher = {Elsevier},
  doi = {https://doi.org/10.1016/j.neucom.2021.08.131},
  abstract = {Dialogue systems have made massive promising progress contributed by deep learning techniques and have been widely applied in our life. However, existing end-to-end neural models suffer from the problem of tending to generate uninformative and generic responses because they cannot ground dialogue context with background knowledge. In order to solve this problem, many researchers begin to consider combining external knowledge in dialogue systems, namely knowledge-enhanced dialogue systems. The challenges of knowledge-enhanced dialogue systems include how to select the appropriate knowledge from large-scale knowledge bases, how to read and understand extracted knowledge, and how to integrate knowledge into responses generation process. Combined with external knowledge, dialogue systems can deeply understand the dialogue context, and generate more informative and logical responses. This survey gives a comprehensive review of knowledge-enhanced dialogue systems, summarizes research progress to solve these challenges and proposes some open issues and research directions.}
}2b:T5bb,In e-commerce platforms, the online descriptive information of products shows significant impacts on the purchase behaviors. To attract potential buyers for product promotion, numerous workers are employed to write the impressive product descriptions. The hand-crafted product descriptions are less-efficient with great labor costs and huge time consumption. Meanwhile, the generated product descriptions do not take consideration into the customization and the diversity to meet users’ interests. To address these problems, we propose one generic framework, namely DeepDepict, to automatically generate the information-rich and personalized product descriptive information. Specifically, DeepDepict leverages the graph attention to retrieve the product-related knowledge from external knowledge base to enrich the diversity of products, constructs the personalized lexicon to capture the linguistic traits of individuals for the personalization of product descriptions, and utilizes multiple pointer-generator network to fuse heterogeneous data from multi-sources to generate informative and personalized product descriptions. We conduct intensive experiments on one public dataset. The experimental results show that DeepDepict outperforms existing solutions in terms of description diversity, BLEU, and personalized degree with significant margin gain, and is able to generate product descriptions with comprehensive knowledge and personalized linguistic traits.2c:T7d7,@article{hao2021deepdepict,
  title = {DeepDepict: enabling information rich, personalized product description generation with the deep multiple pointer generator network},
  author = {Hao, Shaoyang and Guo, Bin and Wang, Hao and Liang, Yunji and Yao, Lina and Wang, Qianru and Yu, Zhiwen},
  journal = {ACM Transactions on Knowledge Discovery from Data (JCR Q1, CCF B)},
  volume = {15},
  number = {5},
  pages = {1--16},
  year = {2021},
  publisher = {ACM New York, NY, USA},
  abstract = {In e-commerce platforms, the online descriptive information of products shows significant impacts on the purchase behaviors. To attract potential buyers for product promotion, numerous workers are employed to write the impressive product descriptions. The hand-crafted product descriptions are less-efficient with great labor costs and huge time consumption. Meanwhile, the generated product descriptions do not take consideration into the customization and the diversity to meet users’ interests. To address these problems, we propose one generic framework, namely DeepDepict, to automatically generate the information-rich and personalized product descriptive information. Specifically, DeepDepict leverages the graph attention to retrieve the product-related knowledge from external knowledge base to enrich the diversity of products, constructs the personalized lexicon to capture the linguistic traits of individuals for the personalization of product descriptions, and utilizes multiple pointer-generator network to fuse heterogeneous data from multi-sources to generate informative and personalized product descriptions. We conduct intensive experiments on one public dataset. The experimental results show that DeepDepict outperforms existing solutions in terms of description diversity, BLEU, and personalized degree with significant margin gain, and is able to generate product descriptions with comprehensive knowledge and personalized linguistic traits.},
  doi = {https://doi.org/10.1145/3446982}
}2d:T439,In recent years, with the development of deep learning, text-generation technology has undergone great changes and provided many kinds of services for human beings, such as restaurant reservation and daily communication. The automatically generated text is becoming more and more fluent so researchers begin to consider more anthropomorphic text-generation technology, that is, the conditional text generation, including emotional text generation, personalized text generation, and so on. Conditional Text Generation (CTG) has thus become a research hotspot. As a promising research field, we find that much attention has been paid to exploring it. Therefore, we aim to give a comprehensive review of the new research trends of CTG. We first summarize several key techniques and illustrate the technical evolution route in the field of neural text generation, based on the concept model of CTG. We further make an investigation of existing CTG fields and propose several general learning models for CTG. Finally, we discuss the open issues and promising research directions of CTG.2e:T60e,@article{guo2021conditional,
  title = {Conditional text generation for harmonious human-machine interaction},
  author = {Guo, Bin and Wang, Hao and Ding, Yasan and Wu, Wei and Hao, Shaoyang and Sun, Yueqi and Yu, Zhiwen},
  journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume = {12},
  number = {2},
  pages = {1--50},
  year = {2021},
  publisher = {ACM New York, NY, USA},
  abstract = {In recent years, with the development of deep learning, text-generation technology has undergone great changes and provided many kinds of services for human beings, such as restaurant reservation and daily communication. The automatically generated text is becoming more and more fluent so researchers begin to consider more anthropomorphic text-generation technology, that is, the conditional text generation, including emotional text generation, personalized text generation, and so on. Conditional Text Generation (CTG) has thus become a research hotspot. As a promising research field, we find that much attention has been paid to exploring it. Therefore, we aim to give a comprehensive review of the new research trends of CTG. We first summarize several key techniques and illustrate the technical evolution route in the field of neural text generation, based on the concept model of CTG. We further make an investigation of existing CTG fields and propose several general learning models for CTG. Finally, we discuss the open issues and promising research directions of CTG.},
  doi = {https://doi.org/10.1145/3439816}
}e:["$","div",null,{"className":"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12","children":[["$","$L18",null,{"config":{"type":"publication","title":"Publications","description":"A collection of my research work.","source":"publications.bib"},"publications":[{"id":"chen2026persuhsg","title":"PersuHSG: Adaptive Persuasion Strategy Planning for Dialogue Agents Based on Hierarchical Strategy Graph","authors":[{"name":"Mengqi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Jingqi Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yan Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yunji Liang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Peilin Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yan Pan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2026,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:0:tags","researchArea":"machine-learning","journal":"ACM Transactions on Information Systems (JCR Q1, CCF A)","conference":"","doi":"x","abstract":"$19","description":"","selected":true,"preview":"TOIS-26.png","bibtex":"$1a"},{"id":"wang2025enabling","title":"Enabling harmonious human-machine interaction with visual-context augmented dialogue system: A review","authors":[{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yating Zeng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Mengqi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yasan Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ying Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lina Yao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:1:tags","researchArea":"machine-learning","journal":"ACM Transactions on Information Systems (JCR Q1, CCF A)","conference":"","volume":"43","issue":"3","pages":"1--59","doi":"https://doi.org/10.1145/3715098","abstract":"$1b","description":"","selected":true,"preview":"TOIS25.jpg","bibtex":"$1c"},{"id":"wu2025hola","title":"HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training","authors":[{"name":"Xuecheng Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Heli Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Danlei Huang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xinyi Yin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yifan Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Jia Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Fei Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Peihao Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Suyu Xing","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"others","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the 33rd ACM International Conference on Multimedia (CCF A Conference)","pages":"13692--13699","doi":"https://doi.org/10.1145/3746027.3761980","abstract":"$1d","description":"","selected":true,"preview":"MM25-HOLA.png","bibtex":"$1e"},{"id":"wang2025cascade","title":"Cascade context-oriented spatio-temporal attention network for efficient and fine-grained video-grounded dialogues","authors":[{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Mengqi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qiuyun Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yasan Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ying Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:3:tags","researchArea":"transformer-architectures","journal":"Frontiers of Computer Science (JCR Q1, CCF B)","conference":"","volume":"19","issue":"7","pages":"197329","doi":"https://doi.org/10.1007/s11704-024-40387-w","abstract":"$1f","description":"","selected":false,"preview":"Architechure_FCS.jpg","bibtex":"$20"},{"id":"chen2025future","title":"The future of cognitive strategy-enhanced persuasive dialogue agents: new perspectives and trends","authors":[{"name":"Mengqi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Haoyu Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qian Zhao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jingqi Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yasan Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yan Pan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:4:tags","researchArea":"machine-learning","journal":"Frontiers of Computer Science (JCR Q1, CCF B)","conference":"","volume":"19","issue":"5","pages":"195315","doi":"https://doi.org/10.1007/s11704-024-40057-x","abstract":"$21","description":"","selected":false,"preview":"Archicture-FCS-chen.png","bibtex":"$22"},{"id":"wang2024memory","title":"Memory-Enhanced Emotional Support Conversations with Motivation-Driven Strategy Inference","authors":[{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Mengqi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yasan Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qiuyun Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ying Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"conference","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:5:tags","researchArea":"machine-learning","journal":"","conference":"Joint European Conference on Machine Learning and Knowledge Discovery in Databases (CCF B Conference)","pages":"213--229","doi":"https://doi.org/10.1007/978-3-031-70362-1_13","abstract":"$23","description":"","selected":true,"preview":"Model_Architecture_ECML.jpg","bibtex":"$24"},{"id":"wang2023towards","title":"Towards informative and diverse dialogue systems over hierarchical crowd intelligence knowledge graph","authors":[{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jiaqi Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yasan Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:6:tags","researchArea":"machine-learning","journal":"ACM Transactions on Knowledge Discovery from Data (JCR Q1, CCF B)","conference":"","volume":"17","issue":"7","pages":"1--25","doi":"https://doi.org/10.1145/3583758","abstract":"$25","description":"","selected":true,"preview":"Architecture-TKDD.png","bibtex":"$26"},{"id":"ding2023piercingeye","title":"PiercingEye: Identifying Both Faint and Distinct Clues for Explainable Fake News Detection with Progressive Dynamic Graph Mining","authors":[{"name":"Yasan Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yan Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Haocheng Shen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"book-chapter","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:7:tags","researchArea":"machine-learning","journal":"","conference":"ECAI (CCF B Conference)","pages":"549--556","doi":"https://doi.org/10.3233/FAIA230315","abstract":"$27","description":"","selected":false,"preview":"FAIA-372-FAIA230315.jpg","bibtex":"$28"},{"id":"wang2021towards","title":"Towards information-rich, logical dialogue systems with knowledge-enhanced neural models","authors":[{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wei Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Sicong Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2021,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:8:tags","researchArea":"neural-networks","journal":"Neurocomputing (JCR Q1, CCF B)","conference":"","volume":"465","pages":"248--264","doi":"https://doi.org/10.1016/j.neucom.2021.08.131","abstract":"$29","description":"","selected":false,"preview":"Neurocomputing.jpg","bibtex":"$2a"},{"id":"hao2021deepdepict","title":"DeepDepict: enabling information rich, personalized product description generation with the deep multiple pointer generator network","authors":[{"name":"Shaoyang Hao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Yunji Liang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lina Yao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qianru Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2021,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:9:tags","researchArea":"machine-learning","journal":"ACM Transactions on Knowledge Discovery from Data (JCR Q1, CCF B)","conference":"","volume":"15","issue":"5","pages":"1--16","doi":"https://doi.org/10.1145/3446982","abstract":"$2b","description":"","selected":false,"preview":"TKDD-Yang.png","bibtex":"$2c"},{"id":"guo2021conditional","title":"Conditional text generation for harmonious human-machine interaction","authors":[{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Yasan Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Wei Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Shaoyang Hao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yueqi Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2021,"type":"journal","status":"published","tags":[],"keywords":"$e:props:children:0:props:publications:10:tags","researchArea":"machine-learning","journal":"ACM Transactions on Intelligent Systems and Technology (TIST)","conference":"","volume":"12","issue":"2","pages":"1--50","doi":"https://doi.org/10.1145/3439816","abstract":"$2d","description":"","selected":false,"bibtex":"$2e"}]}],false,false]}]
14:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
10:null
2f:I[622,[],"IconMark"]
12:{"metadata":[["$","title","0",{"children":"Publications | Hao Wang (王豪)"}],["$","meta","1",{"name":"description","content":"A collection of my research work."}],["$","meta","2",{"name":"author","content":"Hao Wang"}],["$","meta","3",{"name":"keywords","content":"Hao Wang,PhD,Research,Xi'an Jiaotong University"}],["$","meta","4",{"name":"creator","content":"Hao Wang"}],["$","meta","5",{"name":"publisher","content":"Hao Wang"}],["$","meta","6",{"property":"og:title","content":"Hao Wang (王豪)"}],["$","meta","7",{"property":"og:description","content":"Assistant Professor at Xi'an Jiaotong University"}],["$","meta","8",{"property":"og:site_name","content":"Hao Wang's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Hao Wang (王豪)"}],["$","meta","13",{"name":"twitter:description","content":"Assistant Professor at Xi'an Jiaotong University"}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}],["$","$L2f","15",{}]],"error":null,"digest":"$undefined"}
17:"$12:metadata"
