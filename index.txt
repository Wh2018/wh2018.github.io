1:"$Sreact.fragment"
2:I[7558,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-f605a9348e2bb169.js"],"ThemeProvider"]
3:I[9994,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-f605a9348e2bb169.js"],"default"]
4:I[9766,[],""]
5:I[8924,[],""]
b:I[7150,[],""]
:HL["/_next/static/css/63b13758c6860200.css","style"]
0:{"P":null,"b":"KAjfSHlmHNdzJZLHW2tAo","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/63b13758c6860200.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Projects","type":"page","target":"projects","href":"/projects"},{"title":"Teaching","type":"page","target":"teaching","href":"/teaching"},{"title":"Awards","type":"page","target":"awards","href":"/awards"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Hao Wang (ÁéãË±™)","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],"$L6","$L7"]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],"$L8"]}]}]]}]]}],{"children":["__PAGE__","$L9",{},null,false]},null,false],"$La",false]],"m":"$undefined","G":["$b",[]],"s":false,"S":true}
c:I[7923,["185","static/chunks/185-02e5984e85404956.js","619","static/chunks/619-9168df9c2a29b74b.js","599","static/chunks/599-36b43173d762fc47.js","177","static/chunks/app/layout-f605a9348e2bb169.js"],"default"]
d:I[9756,["185","static/chunks/185-02e5984e85404956.js","140","static/chunks/140-5ce7a5cc299b9e70.js","619","static/chunks/619-9168df9c2a29b74b.js","681","static/chunks/681-68a0f630dd3881d4.js","974","static/chunks/app/page-9e053bdfb45cdfbc.js"],"default"]
e:I[470,["185","static/chunks/185-02e5984e85404956.js","140","static/chunks/140-5ce7a5cc299b9e70.js","619","static/chunks/619-9168df9c2a29b74b.js","681","static/chunks/681-68a0f630dd3881d4.js","974","static/chunks/app/page-9e053bdfb45cdfbc.js"],"default"]
f:I[2597,["185","static/chunks/185-02e5984e85404956.js","140","static/chunks/140-5ce7a5cc299b9e70.js","619","static/chunks/619-9168df9c2a29b74b.js","681","static/chunks/681-68a0f630dd3881d4.js","974","static/chunks/app/page-9e053bdfb45cdfbc.js"],"default"]
1c:I[4431,[],"ViewportBoundary"]
1e:I[4431,[],"MetadataBoundary"]
1f:"$Sreact.suspense"
6:["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}]
7:["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]
8:["$","$Lc",null,{"lastUpdated":"December 10, 2025"}]
10:T55b,Persuasion, a vital social skill, influences beliefs, attitudes, and behaviors through conversation. Yet, current dialogue agents either rely on scenario-specific strategies, restricting their cross-context adaptability, or neglect persuasion‚Äôs logical structure. They focus on isolated strategy classification, overlooking the significance of fine-grained sequential planning for real-world scenarios. To address these limitations, inspired by basic human mental activities, we present PersuHSG, an adaptive persuasion strategy planning framework. The core idea is to conceptualize persuasion as a tripartite framework comprising cognition, affection, and volition, with each stage represented as a graph layer and principle-based strategies for efficient multi-stage persuasion. Specifically, we first develop PersuInstruct, a fine-tuning dataset to improve dialogue agents‚Äô strategic planning and response generation. Then, we propose a graph-aware planning algorithm for stage-strategy-response reasoning to generate persuasive responses for diverse scenarios. Extensive experiments confirm that PersuHSG significantly enhances the persuasiveness of large language models (LLMs), allows smaller models (e.g., 9B, 13B) to achieve competitive performance, and demonstrates the efficacy of structured strategy planning in improving model efficiency and adaptability.11:T713,@article{chen2026persuhsg,
  title = {PersuHSG: Adaptive Persuasion Strategy Planning for Dialogue Agents Based on Hierarchical Strategy Graph},
  author = {Chen, Mengqi and Guo, Bin and Wang, Hao and Liu, Jingqi and Liu, Yan and Liang, Yunji and Li, Peilin and Pan, Yan and Yu, Zhiwen},
  journal = {ACM Transactions on Information Systems (JCR Q1, CCF A)},
  year = {2026},
  publisher = {ACM New York, NY},
  abstract = {Persuasion, a vital social skill, influences beliefs, attitudes, and behaviors through conversation. Yet, current dialogue agents either rely on scenario-specific strategies, restricting their cross-context adaptability, or neglect persuasion‚Äôs logical structure. They focus on isolated strategy classification, overlooking the significance of fine-grained sequential planning for real-world scenarios. To address these limitations, inspired by basic human mental activities, we present PersuHSG, an adaptive persuasion strategy planning framework. The core idea is to conceptualize persuasion as a tripartite framework comprising cognition, affection, and volition, with each stage represented as a graph layer and principle-based strategies for efficient multi-stage persuasion. Specifically, we first develop PersuInstruct, a fine-tuning dataset to improve dialogue agents‚Äô strategic planning and response generation. Then, we propose a graph-aware planning algorithm for stage-strategy-response reasoning to generate persuasive responses for diverse scenarios. Extensive experiments confirm that PersuHSG significantly enhances the persuasiveness of large language models (LLMs), allows smaller models (e.g., 9B, 13B) to achieve competitive performance, and demonstrates the efficacy of structured strategy planning in improving model efficiency and adaptability.},
  doi = {x}
}12:T5c8,The intelligent dialogue system, aiming at communicating with humans harmoniously with natural language, is brilliant for promoting the advancement of human-machine interaction in the era of artificial intelligence. With the gradually complex human-computer interaction requirements, it is difficult for traditional text-based dialogue system to meet the demands for more vivid and convenient interaction. Consequently, Visual-Context Augmented Dialogue (VAD) System, which has the potential to communicate with humans by perceiving and understanding multimodal information (i.e., visual context in images or videos, textual dialogue history), has become a predominant research paradigm. Benefiting from the consistency and complementarity between visual and textual context, VAD possesses the potential to generate engaging and context-aware responses. To depict the development of VAD, we first characterize the concept model of VAD and then present its generic system architecture to illustrate the system workflow, followed by a summary of multimodal fusion techniques. Subsequently, several research challenges and representative works are investigated, followed by the summary of authoritative benchmarks and real-world application of VAD. We conclude this article by putting forward some open issues and promising research trends for VAD, e.g., the cognitive mechanisms of human-machine dialogue under cross-modal dialogue context, mobile and lightweight deployment of VAD.13:T7c4,@article{wang2025enabling,
  title = {Enabling harmonious human-machine interaction with visual-context augmented dialogue system: A review},
  author = {Wang, Hao and Guo, Bin and Zeng, Yating and Chen, Mengqi and Ding, Yasan and Zhang, Ying and Yao, Lina and Yu, Zhiwen},
  journal = {ACM Transactions on Information Systems (JCR Q1, CCF A)},
  volume = {43},
  number = {3},
  pages = {1--59},
  year = {2025},
  publisher = {ACM New York, NY},
  abstract = {The intelligent dialogue system, aiming at communicating with humans harmoniously with natural language, is brilliant for promoting the advancement of human-machine interaction in the era of artificial intelligence. With the gradually complex human-computer interaction requirements, it is difficult for traditional text-based dialogue system to meet the demands for more vivid and convenient interaction. Consequently, Visual-Context Augmented Dialogue (VAD) System, which has the potential to communicate with humans by perceiving and understanding multimodal information (i.e., visual context in images or videos, textual dialogue history), has become a predominant research paradigm. Benefiting from the consistency and complementarity between visual and textual context, VAD possesses the potential to generate engaging and context-aware responses. To depict the development of VAD, we first characterize the concept model of VAD and then present its generic system architecture to illustrate the system workflow, followed by a summary of multimodal fusion techniques. Subsequently, several research challenges and representative works are investigated, followed by the summary of authoritative benchmarks and real-world application of VAD. We conclude this article by putting forward some open issues and promising research trends for VAD, e.g., the cognitive mechanisms of human-machine dialogue under cross-modal dialogue context, mobile and lightweight deployment of VAD.},
  doi = {https://doi.org/10.1145/3715098}
}14:T502,Advances in Generative AI have made video-level deepfake detection increasingly challenging, exposing the limitations of current detection techniques. In this paper, we present HOLA, our solution to the Video-Level Deepfake Detection track of 2025 1M-Deepfakes Detection Challenge. Inspired by the success of large-scale pretraining in the general domain, we first scale audio-visual selfsupervised pre-training in the multimodal video-level deepfake detection, which leverages our self-built dataset of 1.81M samples thereby leading to a unified two-stage framework. To be specific, HOLA features an iterative-aware cross-modal learning module for selective audio-visual interactions, hierarchical contextual modeling with gated aggregations under the local-global perspective, and a pyramid-like refiner for scale-aware cross-grained semantic enhancements. Moreover, we propose the pseudo supervised singal injection strategy to further boost model performance. Extensive experiments across expert models and MLLMs impressivly demonstrate the effectiveness of our proposed HOLA. We also conduct a series of ablation studies to explore the crucial design factors of our introduced components. Remarkably, our HOLA ranks 1st, outperforming the second by 0.0476 AUC on the TestA set.15:T723,@inproceedings{wu2025hola,
  title = {HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training},
  author = {Wu, Xuecheng and Sun, Heli and Huang, Danlei and Yin, Xinyi and Wang, Yifan and Wang, Hao and Zhang, Jia and Wang, Fei and Guo, Peihao and Xing, Suyu and others},
  booktitle = {Proceedings of the 33rd ACM International Conference on Multimedia (CCF A Conference)},
  pages = {13692--13699},
  year = {2025},
  abstract = {Advances in Generative AI have made video-level deepfake detection increasingly challenging, exposing the limitations of current detection techniques. In this paper, we present HOLA, our solution to the Video-Level Deepfake Detection track of 2025 1M-Deepfakes Detection Challenge. Inspired by the success of large-scale pretraining in the general domain, we first scale audio-visual selfsupervised pre-training in the multimodal video-level deepfake detection, which leverages our self-built dataset of 1.81M samples thereby leading to a unified two-stage framework. To be specific, HOLA features an iterative-aware cross-modal learning module for selective audio-visual interactions, hierarchical contextual modeling with gated aggregations under the local-global perspective, and a pyramid-like refiner for scale-aware cross-grained semantic enhancements. Moreover, we propose the pseudo supervised singal injection strategy to further boost model performance. Extensive experiments across expert models and MLLMs impressivly demonstrate the effectiveness of our proposed HOLA. We also conduct a series of ablation studies to explore the crucial design factors of our introduced components. Remarkably, our HOLA ranks 1st, outperforming the second by 0.0476 AUC on the TestA set.},
  doi = {https://doi.org/10.1145/3746027.3761980}
}16:T5d5,The prevalence of mental disorders is increasing globally, highlighting the need for effective emotional support systems. Emotional Support Conversation (ESC) systems have emerged as a promising solution, providing supportive responses to individuals experiencing emotional distress. However, existing ESC systems face challenges in deducing appropriate support strategies with motivations and modeling complex semantic patterns inherent in support strategies. To tackle these challenges, we propose MAGIC, a Memory-enhanced emotional support conversation system with motivation-driven strAteGy InferenCe. Considering potential responses in future conversations, we instruct models to deduce motivations for selecting appropriate future strategies from the prior dialogue history, by harnessing the knowledge deducing abilities of Large Language Models (LLMs). These deduced motivations serve as chain-of-thought to steer models in understanding the underlying reasons of strategy inference. Moreover, to capture the intricate human language patterns and knowledge embedded in support strategies, we introduce a strategy memory store to enhance strategy modeling, by disentangling the representations from the same-strategy responses as strategy memory. Experimental results on the ESConv dataset demonstrate that MAGIC significantly outperforms state-of-the-art baselines in both automatic and human evaluations, showcasing its effectiveness in generating empathetic and supportive responses.17:T7d6,@inproceedings{wang2024memory,
  title = {Memory-Enhanced Emotional Support Conversations with Motivation-Driven Strategy Inference},
  author = {Wang, Hao and Guo, Bin and Chen, Mengqi and Ding, Yasan and Zhang, Qiuyun and Zhang, Ying and Yu, Zhiwen},
  booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases (CCF B Conference)},
  pages = {213--229},
  year = {2024},
  organization = {Springer},
  doi = {https://doi.org/10.1007/978-3-031-70362-1_13},
  abstract = {The prevalence of mental disorders is increasing globally, highlighting the need for effective emotional support systems. Emotional Support Conversation (ESC) systems have emerged as a promising solution, providing supportive responses to individuals experiencing emotional distress. However, existing ESC systems face challenges in deducing appropriate support strategies with motivations and modeling complex semantic patterns inherent in support strategies. To tackle these challenges, we propose MAGIC, a Memory-enhanced emotional support conversation system with motivation-driven strAteGy InferenCe. Considering potential responses in future conversations, we instruct models to deduce motivations for selecting appropriate future strategies from the prior dialogue history, by harnessing the knowledge deducing abilities of Large Language Models (LLMs). These deduced motivations serve as chain-of-thought to steer models in understanding the underlying reasons of strategy inference. Moreover, to capture the intricate human language patterns and knowledge embedded in support strategies, we introduce a strategy memory store to enhance strategy modeling, by disentangling the representations from the same-strategy responses as strategy memory. Experimental results on the ESConv dataset demonstrate that MAGIC significantly outperforms state-of-the-art baselines in both automatic and human evaluations, showcasing its effectiveness in generating empathetic and supportive responses.}
}18:T827,Knowledge-enhanced dialogue systems aim at generating actually correct and coherent responses by reasoning over knowledge sources, which is a promising research trend. The truly harmonious human-agent dialogue systems need to conduct engaging conversations from three aspects as humans, namely (1) stating factual contents (e.g., records in Wikipedia), (2) conveying subjective and informative opinions about objects (e.g., user discussions on Twitter), and (3) impressing interlocutors with diverse expression styles (e.g., personalized expression habits). The existing knowledge base is a standardized and unified coding for factual knowledge, which could not portray the other two kinds of knowledge to make responses more informative and expressive diverse. To address this, we present CrowdDialog, a crowd intelligence knowledge-enhanced dialogue system, which takes advantage of ‚Äúcrowd intelligence knowledge‚Äù extracted from social media (with rich subjective descriptions and diversified expression styles) to promote the performance of dialogue systems. Firstly, to thoroughly mine and organize the crowd intelligence knowledge underlying large-scale and unstructured online contents, we elaborately design the Crowd Intelligence Knowledge Graph (CIKG) structure, including the domain commonsense subgraph, descriptive subgraph, and expressive subgraph. Secondly, to reasonably integrate heterogeneous crowd intelligence knowledge into responses while ensuring logicality and fluency, we propose the Gated Fusion with Dynamic Knowledge-Dependent (GFDD) model, which generates responses from the semantic and syntactic perspective with the context-aware knowledge gate and dynamic knowledge decoding. Finally, extensive experiments over both Chinese and English dialogue datasets demonstrate that our approach GFDD outperforms competitive baselines in terms of both automatic evaluation and human judgments. Besides, ablation studies indicate that the proposed CIKG has the potential to promote dialogue systems to generate fluent, informative, and diverse dialogue responses.19:T9fb,@article{wang2023towards,
  title = {Towards informative and diverse dialogue systems over hierarchical crowd intelligence knowledge graph},
  author = {Wang, Hao and Guo, Bin and Liu, Jiaqi and Ding, Yasan and Yu, Zhiwen},
  journal = {ACM Transactions on Knowledge Discovery from Data (JCR Q1, CCF B)},
  volume = {17},
  number = {7},
  pages = {1--25},
  year = {2023},
  publisher = {ACM New York, NY},
  doi = {https://doi.org/10.1145/3583758},
  abstract = {Knowledge-enhanced dialogue systems aim at generating actually correct and coherent responses by reasoning over knowledge sources, which is a promising research trend. The truly harmonious human-agent dialogue systems need to conduct engaging conversations from three aspects as humans, namely (1) stating factual contents (e.g., records in Wikipedia), (2) conveying subjective and informative opinions about objects (e.g., user discussions on Twitter), and (3) impressing interlocutors with diverse expression styles (e.g., personalized expression habits). The existing knowledge base is a standardized and unified coding for factual knowledge, which could not portray the other two kinds of knowledge to make responses more informative and expressive diverse. To address this, we present CrowdDialog, a crowd intelligence knowledge-enhanced dialogue system, which takes advantage of ‚Äúcrowd intelligence knowledge‚Äù extracted from social media (with rich subjective descriptions and diversified expression styles) to promote the performance of dialogue systems. Firstly, to thoroughly mine and organize the crowd intelligence knowledge underlying large-scale and unstructured online contents, we elaborately design the Crowd Intelligence Knowledge Graph (CIKG) structure, including the domain commonsense subgraph, descriptive subgraph, and expressive subgraph. Secondly, to reasonably integrate heterogeneous crowd intelligence knowledge into responses while ensuring logicality and fluency, we propose the Gated Fusion with Dynamic Knowledge-Dependent (GFDD) model, which generates responses from the semantic and syntactic perspective with the context-aware knowledge gate and dynamic knowledge decoding. Finally, extensive experiments over both Chinese and English dialogue datasets demonstrate that our approach GFDD outperforms competitive baselines in terms of both automatic evaluation and human judgments. Besides, ablation studies indicate that the proposed CIKG has the potential to promote dialogue systems to generate fluent, informative, and diverse dialogue responses.}
}9:["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$Ld",null,{"author":{"name":"Hao Wang","title":"Assistant Professor","institution":"Xi'an Jiaotong University","avatar":"/haowang.jpg"},"social":{"email":"haowangx@xjtu.edu.cn","google_scholar":"https://scholar.google.com.hk/citations?user=PeY6HBkAAAAJ&hl=zh-CN","orcid":"https://orcid.org/0000-0001-6959-7237"},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["LLMs","Social Simulation Empowered by Agents","Intelligent Communication","Public Opinion Analysis"]}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$Le","about",{"content":"I am currently an **Assistant Professor** at the **School of Journalism and New Media** at **Xi'an Jiaotong University**. \n\nI am recognized as an **Outstanding Young Talent (ÈùíÂπ¥‰ºòÁßÄ‰∫∫Êâç)** of Xi'an Jiaotong University.\n\nI earned my Ph.D. from the **School of Computer Science** at **Northwestern Polytechnical University** (NWPU). Before that, I received B.E. degree in Computer Science and Technology at NWPU in 2019.\n\nMy research interests include **Large Language Models (LLMs)**, **Social Simulation Empowered by Agents**, **Intelligent Communication**, and **Public Opinion Analysis**.\n\nFeel free to connect!","title":"About"}],["$","$Lf","featured_publications",{"publications":[{"id":"chen2026persuhsg","title":"PersuHSG: Adaptive Persuasion Strategy Planning for Dialogue Agents Based on Hierarchical Strategy Graph","authors":[{"name":"Mengqi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Jingqi Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yan Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yunji Liang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Peilin Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yan Pan","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2026,"type":"journal","status":"published","tags":[],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags","researchArea":"machine-learning","journal":"ACM Transactions on Information Systems (JCR Q1, CCF A)","conference":"","doi":"x","abstract":"$10","description":"","selected":true,"preview":"TOIS-26.png","bibtex":"$11"},{"id":"wang2025enabling","title":"Enabling harmonious human-machine interaction with visual-context augmented dialogue system: A review","authors":[{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yating Zeng","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Mengqi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yasan Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ying Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Lina Yao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"journal","status":"published","tags":[],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags","researchArea":"machine-learning","journal":"ACM Transactions on Information Systems (JCR Q1, CCF A)","conference":"","volume":"43","issue":"3","pages":"1--59","doi":"https://doi.org/10.1145/3715098","abstract":"$12","description":"","selected":true,"preview":"TOIS25.jpg","bibtex":"$13"},{"id":"wu2025hola","title":"HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training","authors":[{"name":"Xuecheng Wu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Heli Sun","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Danlei Huang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xinyi Yin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yifan Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Jia Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Fei Wang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Peihao Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Suyu Xing","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"others","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the 33rd ACM International Conference on Multimedia (CCF A Conference)","pages":"13692--13699","doi":"https://doi.org/10.1145/3746027.3761980","abstract":"$14","description":"","selected":true,"preview":"MM25-HOLA.png","bibtex":"$15"},{"id":"wang2024memory","title":"Memory-Enhanced Emotional Support Conversations with Motivation-Driven Strategy Inference","authors":[{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Mengqi Chen","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yasan Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qiuyun Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Ying Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"conference","status":"published","tags":[],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:3:tags","researchArea":"machine-learning","journal":"","conference":"Joint European Conference on Machine Learning and Knowledge Discovery in Databases (CCF B Conference)","pages":"213--229","doi":"https://doi.org/10.1007/978-3-031-70362-1_13","abstract":"$16","description":"","selected":true,"preview":"Model_Architecture_ECML.jpg","bibtex":"$17"},{"id":"wang2023towards","title":"Towards informative and diverse dialogue systems over hierarchical crowd intelligence knowledge graph","authors":[{"name":"Hao Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Bin Guo","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jiaqi Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yasan Ding","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhiwen Yu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2023,"type":"journal","status":"published","tags":[],"keywords":"$9:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:4:tags","researchArea":"machine-learning","journal":"ACM Transactions on Knowledge Discovery from Data (JCR Q1, CCF B)","conference":"","volume":"17","issue":"7","pages":"1--25","doi":"https://doi.org/10.1145/3583758","abstract":"$18","description":"","selected":true,"preview":"Architecture-TKDD.png","bibtex":"$19"}],"title":"Selected Publications","enableOnePageMode":false}],"$L1a"],false,false,false]}]]}]]}]}],null,"$L1b"]}]
a:["$","$1","h",{"children":[null,[["$","$L1c",null,{"children":"$L1d"}],null],["$","$L1e",null,{"children":["$","div",null,{"hidden":true,"children":["$","$1f",null,{"fallback":null,"children":"$L20"}]}]}]]}]
21:I[5078,["185","static/chunks/185-02e5984e85404956.js","140","static/chunks/140-5ce7a5cc299b9e70.js","619","static/chunks/619-9168df9c2a29b74b.js","681","static/chunks/681-68a0f630dd3881d4.js","974","static/chunks/app/page-9e053bdfb45cdfbc.js"],"default"]
22:I[4431,[],"OutletBoundary"]
24:I[5278,[],"AsyncMetadataOutlet"]
1a:["$","$L21","news",{"items":[{"date":"2026-02","content":"üéâ Our paper is accepted by ACM TOIS, CCF Rank A"},{"date":"2025-08","content":"üéâ We win the first place in 1M-Deepfakes Detection Challenge (https://deepfakes1m.github.io/2025/evaluation) @ ACM Multimedia 2025"},{"date":"2025-08","content":"üéâ Our paper is accepted by ACM Multimedia 2025, CCF Rank A"},{"date":"2025-01","content":"üéâ Our paper is accepted by ACM Transactions on Information Systems 2025, CCF Rank A"}],"title":"News"}]
1b:["$","$L22",null,{"children":["$L23",["$","$L24",null,{"promise":"$@25"}]]}]
1d:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
23:null
26:I[622,[],"IconMark"]
25:{"metadata":[["$","title","0",{"children":"Hao Wang (ÁéãË±™)"}],["$","meta","1",{"name":"description","content":"Assistant Professor at Xi'an Jiaotong University"}],["$","meta","2",{"name":"author","content":"Hao Wang"}],["$","meta","3",{"name":"keywords","content":"Hao Wang,PhD,Research,Xi'an Jiaotong University"}],["$","meta","4",{"name":"creator","content":"Hao Wang"}],["$","meta","5",{"name":"publisher","content":"Hao Wang"}],["$","meta","6",{"property":"og:title","content":"Hao Wang (ÁéãË±™)"}],["$","meta","7",{"property":"og:description","content":"Assistant Professor at Xi'an Jiaotong University"}],["$","meta","8",{"property":"og:site_name","content":"Hao Wang's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Hao Wang (ÁéãË±™)"}],["$","meta","13",{"name":"twitter:description","content":"Assistant Professor at Xi'an Jiaotong University"}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}],["$","$L26","15",{}]],"error":null,"digest":"$undefined"}
20:"$25:metadata"
